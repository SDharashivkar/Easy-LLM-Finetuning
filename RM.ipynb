{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdbbed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_scheduler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0ad38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration stanfordnlp--SHP-c26141d67734c662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/stanfordnlp--SHP to /Users/sachin/.cache/huggingface/datasets/stanfordnlp___json/stanfordnlp--SHP-c26141d67734c662/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3007622ef13a419e86dee452dbebe9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a07512459714c6dbdf2d6c0bfa5d08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/sachin/.cache/huggingface/datasets/stanfordnlp___json/stanfordnlp--SHP-c26141d67734c662/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba15a801d333494eab10aec0d83776b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-410m\")\n",
    "dataset = load_dataset(\"stanfordnlp/SHP\")\n",
    "# dataset = dataset.train_test_split(test_size=0.8)\n",
    "system_prompt = tokenizer.bos_token + \"system\\n The following is a conversation between user and an AI assistant. \" \\\n",
    "                                      \"The assistant is helpful, creative, clever, and very friendly.\\n\" \\\n",
    "                + tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9b400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    dictionary = {}\n",
    "    completion_a = tokenizer(system_prompt + tokenizer.bos_token + \"user: \" + example[\"history\"] + tokenizer.eos_token +\n",
    "                           tokenizer.bos_token + \"assistant: \" + example[\"human_ref_A\"] + tokenizer.eos_token,\n",
    "                           truncation=True)\n",
    "    dictionary[\"input_ids_A\"] = completion_a.pop(\"input_ids\")\n",
    "    dictionary[\"attention_mask_A\"] = completion_a.pop(\"attention_mask\")\n",
    "    completion_b = tokenizer(system_prompt + tokenizer.bos_token + \"user: \" + example[\"history\"] + tokenizer.eos_token +\n",
    "                           tokenizer.bos_token + \"assistant: \" + example[\"human_ref_B\"] + tokenizer.eos_token,\n",
    "                           truncation=True)\n",
    "    dictionary[\"input_ids_B\"] = completion_b.pop(\"input_ids\")\n",
    "    dictionary[\"attention_mask_B\"] = completion_b.pop(\"attention_mask\")\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47728f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=range(0, 10000) of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabf2808917e433b99b55e2edae3a00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(10000))\n",
    "tokenized_dataset = train_dataset.map(tokenize_function)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['post_id', 'domain', 'upvote_ratio', 'history', 'c_root_id_A',\n",
    "                                                      'c_root_id_B', 'created_at_utc_A', 'created_at_utc_B',\n",
    "                                                      'human_ref_A', 'human_ref_B', 'labels', 'seconds_difference',\n",
    "                                                      'score_ratio'])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(tokenized_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8792af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-410m\")\n",
    "        self.linear = nn.Linear(self.base_model.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        base_model_output = self.base_model(input_ids, attention_mask, output_hidden_states=True)\n",
    "        #We take mean of reward score for every token's last layer hidden state.\n",
    "        #Another valid startegy can be to only consider reward score for the last token's last layer hidden state.\n",
    "        return torch.mean(self.linear(base_model_output.hidden_states[-1].detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11701491",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model = RewardModel()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "reward_model.to(device)\n",
    "optimizer = AdamW(reward_model.linear.parameters(), lr=1e-5)\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be4178fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model.train()\n",
    "i = 0\n",
    "running_loss = 0.0\n",
    "training_loss = []\n",
    "model_save_path = 'rm-pythia.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bb281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "0.7143135058879853\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "0.7218931090831756\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "0.6825545197725296\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    i += 1\n",
    "    print(i)\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    model_score_A = reward_model(input_ids=batch[\"input_ids_A\"], attention_mask=batch[\"attention_mask_A\"])\n",
    "    model_score_B = reward_model(input_ids=batch[\"input_ids_B\"], attention_mask=batch[\"attention_mask_B\"])\n",
    "    if batch[\"score_A\"] > batch[\"score_B\"]:\n",
    "        loss = -1 * nn.functional.logsigmoid(model_score_A - model_score_B)\n",
    "    else:\n",
    "        loss = -1 * nn.functional.logsigmoid(model_score_B - model_score_A)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    running_loss += loss.item()\n",
    "    if i % 50 == 0:\n",
    "        print(running_loss / 50)\n",
    "        training_loss.append(running_loss / 50)\n",
    "        running_loss = 0.0\n",
    "        torch.save(reward_model.linear.state_dict(), model_save_path)\n",
    "        i = 0\n",
    "torch.save(reward_model.linear.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4b0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
